{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_hinge_loss(outputs):\n",
    "    return -torch.mean(outputs)\n",
    "\n",
    "def discriminator_real_hinge_loss(outputs):\n",
    "    zeros = torch.zeros(len(outputs), 1)\n",
    "    minval = torch.min(outputs - 1, zeros)\n",
    "    return -torch.mean(minval)\n",
    "\n",
    "def discriminator_fake_hinge_loss(outputs):\n",
    "    zeros = torch.zeros(len(outputs), 1)\n",
    "    minval = torch.min(-outputs - 1, zeros)\n",
    "    return - torch.mean(minval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        # image size = 1 * 20 * 20 = 400\n",
    "        # self.image_size = 14\n",
    "        self.image_size = 20\n",
    "        self.batch_size = 1\n",
    "        # latent size\n",
    "        latent_size = 16\n",
    "        self.fc1 = nn.Linear(self.image_size * self.image_size, 64)\n",
    "        self.fc2 = nn.Linear(64, latent_size)\n",
    "        self.fc3 = nn.Linear(latent_size, 64)\n",
    "        self.fc4 = nn.Linear(64, self.image_size * self.image_size)\n",
    "        self.elu = nn.ReLU(.5)\n",
    "\n",
    "        self.latent = None\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.batch_size = x.size(0)\n",
    "        x = x.view(self.batch_size, -1)\n",
    "        # print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.elu(x)\n",
    "        # print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        x = self.elu(x)\n",
    "        self.latent = x\n",
    "        return x\n",
    "\n",
    "    def decode(self,x):\n",
    "        x = self.fc3(x)\n",
    "        # print(x.shape)\n",
    "        x = self.elu(x)\n",
    "        x = self.fc4(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(self.batch_size, 1, self.image_size, self.image_size)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        # print(x.shape)\n",
    "        x = self.decode(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aligner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Aligner, self).__init__()\n",
    "        # self.image_size = 14\n",
    "        self.image_size = 20\n",
    "        self.batch_size = 1\n",
    "        # read layer\n",
    "        self.fc1 = nn.Linear(self.image_size * self.image_size, self.image_size * self.image_size)\n",
    "\n",
    "        # exp unit\n",
    "        self.relu = nn.ReLU(.5)\n",
    "\n",
    "        # out layer\n",
    "        self.fc2 = nn.Linear(self.image_size * self.image_size, self.image_size * self.image_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.batch_size = x.size(0)\n",
    "        x = x.view(self.batch_size, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(self.batch_size, 1, self.image_size, self.image_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 120)\n",
    "        #self.fc1 = nn.Linear(16, 12)\n",
    "        self.fc2 = nn.Linear(120, 12)\n",
    "        self.relu = nn.ReLU(.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vaf(x, y):\n",
    "    x_dev = x - torch.mean(x,dim=0)\n",
    "    y_dev = y - torch.mean(y,dim=0)\n",
    "    return 1 - torch.sum(torch.square(x_dev - y_dev)) / torch.sum(torch.square(x_dev))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def pairwise_distance(x, y):\n",
    "\n",
    "    if not len(x.shape) == len(y.shape) == 2:\n",
    "        raise ValueError('Both inputs should be matrices.')\n",
    "\n",
    "    if x.shape[1] != y.shape[1]:\n",
    "        raise ValueError('The number of features should be the same.')\n",
    "\n",
    "    x = x.view(x.shape[0], x.shape[1], 1)\n",
    "    y = torch.transpose(y, 0, 1)\n",
    "    output = torch.sum((x - y) ** 2, 1)\n",
    "    output = torch.transpose(output, 0, 1)\n",
    "\n",
    "    return output\n",
    "\n",
    "def gaussian_kernel_matrix(x, y, sigmas):\n",
    "\n",
    "    sigmas = sigmas.view(sigmas.shape[0], 1)\n",
    "    beta = 1. / (2. * sigmas)\n",
    "    dist = pairwise_distance(x, y).contiguous()\n",
    "    dist_ = dist.view(1, -1)\n",
    "    s = torch.matmul(beta, dist_)\n",
    "\n",
    "    return torch.sum(torch.exp(-s), 0).view_as(dist)\n",
    "\n",
    "def maximum_mean_discrepancy(x, y, kernel= gaussian_kernel_matrix):\n",
    "\n",
    "    cost = torch.mean(kernel(x, x))\n",
    "    cost += torch.mean(kernel(y, y))\n",
    "    cost -= 2 * torch.mean(kernel(x, y))\n",
    "\n",
    "    return cost\n",
    "\n",
    "def mmd_loss(source_features, target_features):\n",
    "\n",
    "    sigmas = [\n",
    "        1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 5, 10, 15, 20, 25, 30, 35, 100,\n",
    "        1e3, 1e4, 1e5, 1e6\n",
    "    ]\n",
    "    \n",
    "    gaussian_kernel = partial(\n",
    "            gaussian_kernel_matrix, sigmas = Variable(torch.FloatTensor(sigmas))\n",
    "        )\n",
    "    loss_value = maximum_mean_discrepancy(source_features, target_features, kernel= gaussian_kernel)\n",
    "    loss_value = loss_value\n",
    "\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Define Model ------- #\n",
    "\n",
    "AE_model = AE()\n",
    "classifier = Classifier()\n",
    "aligner = Aligner()\n",
    "discriminator = AE()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Optimizer ------- #\n",
    "\n",
    "\n",
    "#AE_optimiser = optim.Adam(AE_model.parameters(), lr=0.001)\n",
    "\n",
    "AE_optimiser = optim.SGD(AE_model.parameters(),lr= 0.001, momentum= 0.9)\n",
    "\n",
    "classifier_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#classifier_optimiser = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "classifier_optimiser = optim.SGD(classifier.parameters(),lr= 0.001, momentum= 0.9)\n",
    "\n",
    "#aligner_optimiser = optim.Adam(aligner.parameters(), lr=0.0002)\n",
    "aligner_optimiser = optim.SGD(aligner.parameters(), lr=0.0002, momentum= 0.9)\n",
    "\n",
    "#discriminator_optimiser = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "discriminator_optimiser = optim.SGD(discriminator.parameters(), lr=0.0002, momentum= 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_criterion = nn.MSELoss()\n",
    "classifier_criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/andy42i/Documents/Documentos/Python/datasets_ADAN/RFID_Activity_Location_PIDLabels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c333a4f7f08e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mp_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdataset_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_hot_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msource_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDAY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtarget_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDAY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\load_dataset_ADAN.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(normalize, one_hot_label, flatten, raw, cache)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \"\"\"\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m         \u001b[0msave_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0msave_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\load_dataset_ADAN.py\u001b[0m in \u001b[0;36msave_pickle\u001b[1;34m(raw)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_from_csv_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_from_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating raw data pickle file ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\load_dataset_ADAN.py\u001b[0m in \u001b[0;36mimport_from_csv\u001b[1;34m()\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[1;34m\"unix_timestamp\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     }\n\u001b[1;32m--> 423\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{dataset_dir}/RFID_Activity_Location_PIDLabels.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/andy42i/Documents/Documentos/Python/datasets_ADAN/RFID_Activity_Location_PIDLabels.csv'"
     ]
    }
   ],
   "source": [
    "from load_dataset_ADAN import load_dataset, get_data_by_ids, PID, get_key_from_val, DAY, DAY_NAME\n",
    "\n",
    "p_rate=0.02\n",
    "\n",
    "dataset_np = load_dataset(one_hot_label=False, cache=False)\n",
    "source_id = DAY[0]\n",
    "target_id = DAY[1]\n",
    "    \n",
    "source_dataset = get_data_by_ids(dataset_np,source_id)\n",
    "source_test_size = 0.5\n",
    "\n",
    "target_dataset = get_data_by_ids(dataset_np,target_id)\n",
    "target_test_size = 0.5\n",
    "target_p_size = p_rate\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# prepare target data\n",
    "target_train_images, target_test_images, target_train_labels, target_test_labels = train_test_split(target_dataset['image'],target_dataset['location_label'], test_size=target_test_size, stratify=target_dataset['location_label'])\n",
    "target_p_train_images, target_p_test_images, target_p_train_labels, target_p_test_labels = train_test_split(target_train_images,target_train_labels, test_size=(1-target_p_size), stratify=target_train_labels)\n",
    "\n",
    "# Prepare source data\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(source_dataset['image'],source_dataset['location_label'], test_size=source_test_size, stratify=source_dataset['location_label'])\n",
    "train_images = np.concatenate([source_dataset['image'],target_p_train_images],axis=0)\n",
    "train_labels = np.concatenate([source_dataset['location_label'],target_p_train_labels],axis=0)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_images).float(),torch.from_numpy(train_labels.astype(np.int64)))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(test_images).float(),torch.from_numpy(test_labels.astype(np.int64)))\n",
    "\n",
    "target_train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(target_p_train_images).float(),torch.from_numpy(target_p_train_labels.astype(np.int64)))\n",
    "target_test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(target_test_images).float(),torch.from_numpy(target_test_labels.astype(np.int64)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041666666666666664\n",
      "0.125\n",
      "0.22916666666666666\n",
      "0.1875\n",
      "0.125\n",
      "0.25\n",
      "0.2708333333333333\n",
      "0.3125\n",
      "0.3541666666666667\n",
      "0.4166666666666667\n",
      "0.3333333333333333\n",
      "0.4791666666666667\n",
      "0.3541666666666667\n",
      "0.5416666666666666\n",
      "0.6041666666666666\n",
      "0.625\n",
      "0.5833333333333334\n",
      "0.4791666666666667\n",
      "0.5833333333333334\n",
      "0.5625\n",
      "0.6666666666666666\n",
      "0.625\n",
      "0.5833333333333334\n",
      "0.6458333333333334\n",
      "0.7083333333333334\n",
      "0.6041666666666666\n",
      "0.625\n",
      "0.5\n",
      "0.6458333333333334\n",
      "0.6041666666666666\n",
      "0.7083333333333334\n",
      "0.6041666666666666\n",
      "0.6458333333333334\n",
      "0.6458333333333334\n",
      "0.6875\n",
      "0.6875\n",
      "0.6041666666666666\n",
      "0.6875\n",
      "0.7291666666666666\n",
      "0.625\n",
      "0.7083333333333334\n",
      "0.6666666666666666\n",
      "0.7708333333333334\n",
      "0.6875\n",
      "0.6875\n",
      "0.6875\n",
      "0.8125\n",
      "0.7291666666666666\n",
      "0.75\n",
      "0.6458333333333334\n",
      "0.75\n",
      "0.7083333333333334\n",
      "0.7291666666666666\n",
      "0.6458333333333334\n",
      "0.625\n",
      "0.6458333333333334\n",
      "0.7291666666666666\n",
      "0.6458333333333334\n",
      "0.6458333333333334\n",
      "0.625\n",
      "0.6041666666666666\n",
      "0.6666666666666666\n",
      "0.7291666666666666\n",
      "0.6666666666666666\n",
      "0.625\n",
      "0.6458333333333334\n",
      "0.625\n",
      "0.625\n",
      "0.7083333333333334\n",
      "0.7291666666666666\n",
      "0.7708333333333334\n",
      "0.7708333333333334\n",
      "0.7291666666666666\n",
      "0.7708333333333334\n",
      "0.7291666666666666\n",
      "0.75\n",
      "0.75\n",
      "0.5416666666666666\n",
      "0.6666666666666666\n",
      "0.6041666666666666\n",
      "0.6666666666666666\n",
      "0.7708333333333334\n",
      "0.7083333333333334\n",
      "0.6458333333333334\n",
      "0.625\n",
      "0.6666666666666666\n",
      "0.6041666666666666\n",
      "0.5625\n",
      "0.7083333333333334\n",
      "0.5208333333333334\n",
      "0.6875\n",
      "0.5833333333333334\n",
      "0.375\n",
      "0.4375\n",
      "0.2916666666666667\n",
      "0.4583333333333333\n",
      "0.4791666666666667\n",
      "0.5416666666666666\n",
      "0.5\n",
      "0.5208333333333334\n",
      "0.4166666666666667\n",
      "0.4791666666666667\n",
      "0.4583333333333333\n",
      "0.3958333333333333\n",
      "0.4166666666666667\n",
      "0.4791666666666667\n",
      "0.4166666666666667\n",
      "0.7083333333333334\n",
      "0.5208333333333334\n",
      "0.6666666666666666\n",
      "0.625\n",
      "0.6041666666666666\n",
      "0.6666666666666666\n",
      "0.7083333333333334\n",
      "0.6458333333333334\n",
      "0.625\n",
      "0.6875\n",
      "0.6458333333333334\n",
      "0.7291666666666666\n",
      "0.7083333333333334\n",
      "0.6458333333333334\n",
      "0.6041666666666666\n",
      "0.6666666666666666\n",
      "0.6875\n",
      "0.7291666666666666\n",
      "0.7083333333333334\n",
      "0.625\n",
      "0.7708333333333334\n",
      "0.6458333333333334\n",
      "0.6875\n",
      "0.7291666666666666\n",
      "0.7916666666666666\n",
      "0.6666666666666666\n",
      "0.75\n",
      "0.8333333333333334\n",
      "0.6458333333333334\n",
      "0.7083333333333334\n",
      "0.7083333333333334\n",
      "0.7083333333333334\n",
      "0.7083333333333334\n",
      "0.6041666666666666\n",
      "0.75\n",
      "0.7708333333333334\n",
      "0.6875\n",
      "0.6458333333333334\n",
      "0.6666666666666666\n",
      "0.7083333333333334\n",
      "0.6666666666666666\n",
      "0.7916666666666666\n",
      "0.7083333333333334\n",
      "0.6666666666666666\n",
      "0.7291666666666666\n",
      "0.8125\n",
      "0.7083333333333334\n",
      "0.7291666666666666\n",
      "0.7291666666666666\n",
      "0.7291666666666666\n",
      "0.6875\n",
      "0.7708333333333334\n",
      "0.6875\n",
      "0.6875\n",
      "0.8333333333333334\n",
      "0.7916666666666666\n",
      "0.7708333333333334\n",
      "0.75\n",
      "0.7291666666666666\n",
      "0.75\n",
      "0.5833333333333334\n",
      "0.625\n",
      "0.75\n",
      "0.5208333333333334\n",
      "0.5208333333333334\n",
      "0.5625\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.7291666666666666\n",
      "0.75\n",
      "0.7916666666666666\n",
      "0.75\n",
      "0.8541666666666666\n",
      "0.7708333333333334\n",
      "0.6666666666666666\n",
      "0.6458333333333334\n",
      "0.7083333333333334\n",
      "0.7708333333333334\n",
      "0.6458333333333334\n",
      "0.5833333333333334\n",
      "0.6041666666666666\n",
      "0.6458333333333334\n",
      "0.4166666666666667\n",
      "0.5\n",
      "0.5416666666666666\n",
      "0.5208333333333334\n",
      "0.5208333333333334\n",
      "0.4583333333333333\n",
      "0.625\n",
      "0.6458333333333334\n",
      "0.6875\n",
      "0.7083333333333334\n",
      "0.7708333333333334\n",
      "0.7708333333333334\n",
      "0.8333333333333334\n",
      "0.75\n",
      "0.7291666666666666\n",
      "0.8541666666666666\n",
      "0.8541666666666666\n",
      "0.8541666666666666\n",
      "0.7916666666666666\n",
      "0.7916666666666666\n",
      "0.8125\n",
      "0.75\n",
      "0.8125\n",
      "0.8125\n",
      "0.6666666666666666\n",
      "0.7708333333333334\n",
      "0.8333333333333334\n",
      "0.8125\n",
      "0.75\n",
      "0.8333333333333334\n",
      "0.7916666666666666\n",
      "0.75\n",
      "0.75\n",
      "0.7291666666666666\n",
      "0.7708333333333334\n",
      "0.8333333333333334\n",
      "0.7916666666666666\n",
      "0.8125\n",
      "0.8541666666666666\n",
      "0.8125\n",
      "0.7916666666666666\n",
      "0.8125\n",
      "0.8125\n",
      "0.8125\n",
      "0.8541666666666666\n",
      "0.8125\n",
      "0.7083333333333334\n",
      "0.7708333333333334\n",
      "0.625\n",
      "0.6666666666666666\n",
      "0.5833333333333334\n",
      "0.7083333333333334\n",
      "0.6875\n",
      "0.7708333333333334\n",
      "0.6458333333333334\n",
      "0.7291666666666666\n",
      "0.6875\n",
      "0.7916666666666666\n",
      "0.8125\n",
      "0.7916666666666666\n",
      "0.7291666666666666\n",
      "0.8333333333333334\n",
      "0.8541666666666666\n",
      "0.7708333333333334\n",
      "0.8541666666666666\n",
      "0.7291666666666666\n",
      "0.7916666666666666\n",
      "0.7291666666666666\n",
      "0.7708333333333334\n",
      "0.7708333333333334\n",
      "0.8333333333333334\n",
      "0.875\n",
      "0.75\n",
      "0.8541666666666666\n",
      "0.7708333333333334\n",
      "0.7291666666666666\n",
      "0.8125\n",
      "0.8125\n",
      "0.8125\n",
      "0.75\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8125\n",
      "0.8958333333333334\n",
      "0.8333333333333334\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8333333333333334\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.7916666666666666\n",
      "0.8125\n",
      "0.8125\n",
      "0.8541666666666666\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.8541666666666666\n",
      "0.7916666666666666\n",
      "0.8333333333333334\n",
      "0.6666666666666666\n",
      "0.6458333333333334\n",
      "0.7916666666666666\n",
      "0.8125\n",
      "0.8333333333333334\n",
      "0.5624336752741422\n",
      "0.5903784931022286\n",
      "0.7083333333333334\n",
      "0.6875\n",
      "0.6041666666666666\n",
      "0.75\n",
      "0.7708333333333334\n",
      "0.7916666666666666\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.875\n",
      "0.8333333333333334\n",
      "0.8541666666666666\n",
      "0.8125\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.875\n",
      "0.9375\n",
      "0.8541666666666666\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.9375\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.875\n",
      "0.8333333333333334\n",
      "0.7916666666666666\n",
      "0.75\n",
      "0.875\n",
      "0.8125\n",
      "0.875\n",
      "0.8125\n",
      "0.7083333333333334\n",
      "0.8333333333333334\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.8333333333333334\n",
      "0.875\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.8541666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.875\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.875\n",
      "0.8125\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.7708333333333334\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.7916666666666666\n",
      "0.9375\n",
      "0.8125\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.7916666666666666\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.7708333333333334\n",
      "0.8125\n",
      "0.7708333333333334\n",
      "0.75\n",
      "0.8333333333333334\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.8333333333333334\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.9166666666666666\n",
      "0.8541666666666666\n",
      "0.9166666666666666\n",
      "0.8125\n",
      "0.8125\n",
      "0.875\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.7708333333333334\n",
      "0.7916666666666666\n",
      "0.8333333333333334\n",
      "0.8958333333333334\n",
      "0.8125\n",
      "0.7916666666666666\n",
      "0.7916666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.875\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.9375\n",
      "0.8125\n",
      "0.6458333333333334\n",
      "0.6458333333333334\n",
      "0.5833333333333334\n",
      "0.7083333333333334\n",
      "0.75\n",
      "0.7083333333333334\n",
      "0.6666666666666666\n",
      "0.75\n",
      "0.7083333333333334\n",
      "0.7916666666666666\n",
      "0.7916666666666666\n",
      "0.875\n",
      "0.8125\n",
      "0.8958333333333334\n",
      "0.625\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.7916666666666666\n",
      "0.75\n",
      "0.8333333333333334\n",
      "0.7916666666666666\n",
      "0.875\n",
      "0.8333333333333334\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.8333333333333334\n",
      "0.875\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.8125\n",
      "0.8958333333333334\n",
      "0.8125\n",
      "0.8125\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.7708333333333334\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8333333333333334\n",
      "0.8125\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.8958333333333334\n",
      "0.8333333333333334\n",
      "0.9166666666666666\n",
      "0.7291666666666666\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.7083333333333334\n",
      "0.8541666666666666\n",
      "0.8125\n",
      "0.875\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.8541666666666666\n",
      "0.7916666666666666\n",
      "0.9166666666666666\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.8125\n",
      "0.8541666666666666\n",
      "0.7708333333333334\n",
      "0.8333333333333334\n",
      "0.8125\n",
      "0.8541666666666666\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.8333333333333334\n",
      "0.8541666666666666\n",
      "0.7708333333333334\n",
      "0.7708333333333334\n",
      "0.875\n",
      "0.8125\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.8125\n",
      "0.8125\n",
      "0.8958333333333334\n",
      "0.8333333333333334\n",
      "0.7291666666666666\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.8333333333333334\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.8333333333333334\n",
      "0.8125\n",
      "0.875\n",
      "0.7916666666666666\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.875\n",
      "0.9166666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9791666666666666\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.8541666666666666\n",
      "0.8958333333333334\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.7291666666666666\n",
      "0.7083333333333334\n",
      "0.7708333333333334\n",
      "0.75\n",
      "0.8125\n",
      "0.875\n",
      "0.8125\n",
      "0.8125\n",
      "0.8125\n",
      "0.7708333333333334\n",
      "0.7291666666666666\n",
      "0.7708333333333334\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.8125\n",
      "0.9166666666666666\n",
      "0.8541666666666666\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.8541666666666666\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.5571276972055182\n",
      "0.5981605942695437\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8125\n",
      "0.8333333333333334\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.7916666666666666\n",
      "0.8541666666666666\n",
      "0.8541666666666666\n",
      "0.9375\n",
      "0.875\n",
      "0.8333333333333334\n",
      "0.9375\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9375\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.8333333333333334\n",
      "0.7916666666666666\n",
      "0.8125\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.875\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.8541666666666666\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.875\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9375\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.8333333333333334\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.7916666666666666\n",
      "0.8333333333333334\n",
      "0.7708333333333334\n",
      "0.6875\n",
      "0.7916666666666666\n",
      "0.7708333333333334\n",
      "0.7708333333333334\n",
      "0.8333333333333334\n",
      "0.8125\n",
      "0.7916666666666666\n",
      "0.9375\n",
      "0.7916666666666666\n",
      "0.8541666666666666\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.8333333333333334\n",
      "0.7916666666666666\n",
      "0.7291666666666666\n",
      "0.8125\n",
      "0.8125\n",
      "0.7291666666666666\n",
      "0.7083333333333334\n",
      "0.8125\n",
      "0.8541666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8333333333333334\n",
      "0.7291666666666666\n",
      "0.8125\n",
      "0.7708333333333334\n",
      "0.8541666666666666\n",
      "0.8125\n",
      "0.8333333333333334\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.8125\n",
      "0.7708333333333334\n",
      "0.75\n",
      "0.8125\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.8125\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.8541666666666666\n",
      "0.8541666666666666\n",
      "0.8958333333333334\n",
      "0.8125\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.8333333333333334\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.8333333333333334\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.7916666666666666\n",
      "0.7291666666666666\n",
      "0.7916666666666666\n",
      "0.8541666666666666\n",
      "0.8125\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.8125\n",
      "0.8125\n",
      "0.7916666666666666\n",
      "0.7916666666666666\n",
      "0.875\n",
      "0.7708333333333334\n",
      "0.7916666666666666\n",
      "0.7916666666666666\n",
      "0.875\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.8541666666666666\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.5758754863813229\n",
      "0.6229218252564556\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.8541666666666666\n",
      "0.9583333333333334\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.875\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.8541666666666666\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.9166666666666666\n",
      "0.7916666666666666\n",
      "0.875\n",
      "0.8125\n",
      "0.875\n",
      "0.8125\n",
      "0.9166666666666666\n",
      "0.7708333333333334\n",
      "0.9375\n",
      "0.8541666666666666\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.875\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.8125\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.875\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "1.0\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.9166666666666666\n",
      "0.7708333333333334\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9166666666666666\n",
      "0.9791666666666666\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "0.8333333333333334\n",
      "0.8958333333333334\n",
      "0.6875\n",
      "0.75\n",
      "0.7708333333333334\n",
      "0.9166666666666666\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.8333333333333334\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.875\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.8958333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.8333333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9166666666666666\n",
      "0.9791666666666666\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.5175097276264592\n",
      "0.6027591085956845\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.8125\n",
      "0.8541666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.875\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.8125\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.8125\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "1.0\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "1.0\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.8541666666666666\n",
      "0.9375\n",
      "0.8333333333333334\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.8541666666666666\n",
      "0.9791666666666666\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.8333333333333334\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.875\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.875\n",
      "0.875\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9166666666666666\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.9166666666666666\n",
      "0.8125\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.9166666666666666\n",
      "0.875\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.875\n",
      "0.7916666666666666\n",
      "0.8541666666666666\n",
      "0.8333333333333334\n",
      "0.875\n",
      "0.875\n",
      "0.8541666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.8541666666666666\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.8958333333333334\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.9375\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "0.9166666666666666\n",
      "0.8958333333333334\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.8958333333333334\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.9375\n",
      "0.9375\n",
      "0.9583333333333334\n",
      "0.9166666666666666\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.8541666666666666\n",
      "0.8958333333333334\n",
      "0.875\n",
      "0.9375\n",
      "0.9166666666666666\n",
      "0.8333333333333334\n",
      "0.9583333333333334\n",
      "0.8125\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9791666666666666\n",
      "0.9583333333333334\n",
      "0.9375\n",
      "1.0\n",
      "0.9791666666666666\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.9791666666666666\n",
      "1.0\n",
      "0.8958333333333334\n",
      "0.9166666666666666\n",
      "0.5737530951538734\n",
      "0.6108949416342413\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=2, batch_size=16, shuffle=True, drop_last=True)\n",
    "target_train_loader = torch.utils.data.DataLoader(target_train_dataset, num_workers=2, batch_size=16, shuffle=True, drop_last=True)\n",
    "    \n",
    "fixed_ae_train_losses = []\n",
    "total_train_losses = []\n",
    "fixed_ae_test_losses = []\n",
    "total_test_losses = []\n",
    "cl_tr_losses = []\n",
    "cl_tr_accuracy = []\n",
    "cl_test_losses = []\n",
    "cl_test_accuracy = []\n",
    "overall_accuracy = []\n",
    "NUM_EXP = 5\n",
    "\n",
    "for ne in range(NUM_EXP):\n",
    "    # Initialize adjust_loss\n",
    "    adjust_loss = torch.tensor(1, dtype=torch.float)\n",
    "    for epoch in range(300):  # loop over the dataset multiple times\n",
    "        total_tr_losses = []\n",
    "        cl_tr_loss = []\n",
    "        ae_tr_loss = []\n",
    "        size = 0\n",
    "        correct = 0\n",
    "        ae_running_loss = 0.0\n",
    "        total_running_loss = 0.0\n",
    "\n",
    "        # Training\n",
    "        AE_model.train()\n",
    "        classifier.train()\n",
    "        aligner.train()\n",
    "        #discriminator.train()\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "        #for i in range(1):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            _adjust_loss = adjust_loss\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            AE_optimiser.zero_grad()\n",
    "            classifier_optimiser.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            # calc auto encoder loss\n",
    "            outputs = AE_model(inputs)\n",
    "            ae_loss = AE_criterion(inputs, outputs)\n",
    "\n",
    "            # calc classifier loss\n",
    "            cl_ae_outputs = AE_model(inputs)\n",
    "            cl_ae_loss = AE_criterion(inputs, outputs)\n",
    "            ae_tr_loss.append(cl_ae_loss)\n",
    "\n",
    "            cl_outputs = classifier(AE_model.latent.clone())\n",
    "            cl_loss = classifier_criterion(cl_outputs, labels)\n",
    "            cl_tr_loss.append(cl_loss)\n",
    "\n",
    "            # calc total loss\n",
    "            total_loss = _adjust_loss * cl_ae_loss + cl_loss\n",
    "            total_tr_losses.append(total_loss)\n",
    "            total_loss.backward()\n",
    "\n",
    "            AE_optimiser.step()\n",
    "            classifier_optimiser.step()\n",
    "\n",
    "            ae_running_loss += ae_loss.item()\n",
    "            total_running_loss += total_loss.item()\n",
    "\n",
    "            _, predicted = cl_outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            size += labels.size(0)\n",
    "\n",
    "            ae_running_loss = 0.0\n",
    "            total_running_loss = 0.0\n",
    "\n",
    "\n",
    "    ###################\n",
    "    model = copy.deepcopy(AE_model.state_dict())\n",
    "    discriminator.load_state_dict(model)\n",
    "    ones = torch.tensor(1, dtype=torch.float)\n",
    "    minus_ones = ones * -1\n",
    "\n",
    "    theta1 = 1\n",
    "    theta2 = 0.5\n",
    "\n",
    "    aligner_train_losses = []\n",
    "    aligner_train_accuracy = []\n",
    "    aligner_test_losses = []\n",
    "    aligner_test_accuracy = []\n",
    "\n",
    "    for epoch in range(300):  # loop over the dataset multiple times\n",
    "        tr_aligner_losses = []\n",
    "        tr_discriminator_losses = []\n",
    "        tr_classifier_losses = []\n",
    "        test_classifier_losses = []\n",
    "        size = 0\n",
    "        correct = 0\n",
    "        AE_model.train()\n",
    "        classifier.train()\n",
    "        aligner.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        target_train_loader = torch.utils.data.DataLoader(target_train_dataset, num_workers=2, batch_size=16, shuffle=True, drop_last=True)\n",
    "\n",
    "        for i, data in enumerate(target_train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Train aligner\n",
    "            aligner_optimiser.zero_grad()\n",
    "            discriminator_optimiser.zero_grad()\n",
    "            aligned = aligner(inputs)\n",
    "            aligned_img_tensor = aligned.detach()\n",
    "            outputs_align = discriminator(aligned)\n",
    "\n",
    "            # classified\n",
    "            ae_out = AE_model(aligned)\n",
    "            cl_outputs = classifier(AE_model.latent.clone())\n",
    "            cl_loss = classifier_criterion(cl_outputs, labels)\n",
    "            tr_classifier_losses.append(cl_loss)\n",
    "            _, predicted = cl_outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            size += labels.size(0)\n",
    "\n",
    "            # loss\n",
    "            output_aligned_r = outputs_align.view(outputs_align.shape[0],outputs_align.shape[2]**2)\n",
    "            aligned_r = aligned.view(aligned.shape[0],aligned.shape[2]**2)\n",
    "\n",
    "            mmdloss = cl_loss*theta1 + mmd_loss(output_aligned_r.float(),aligned_r.float()) * theta2 # + mmd_loss(predicted.float(), labels.float()) * theta2\n",
    "\n",
    "            #loss_align = cl_loss + torch.mean((torch.abs(outputs_align - aligned)))\n",
    "\n",
    "            loss_align = mmdloss\n",
    "\n",
    "            tr_aligner_losses.append(loss_align)\n",
    "            loss_align.backward(retain_graph=True)\n",
    "            aligner_optimiser.step()\n",
    "\n",
    "            # Train discriminator\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            discriminator_optimiser.zero_grad()\n",
    "            aligner_optimiser.zero_grad()\n",
    "\n",
    "            # load source data\n",
    "            orig_dataset_iter = iter(train_loader)\n",
    "            originals = orig_dataset_iter.next()[0]\n",
    "            outputs_orig = discriminator(originals)\n",
    "            outputs_align = discriminator(aligned_img_tensor)\n",
    "\n",
    "            originals_r = originals.view(originals.shape[0],originals.shape[2]**2)\n",
    "            outputs_orig_r = outputs_orig.view(outputs_orig.shape[0],outputs_orig.shape[2]**2)\n",
    "\n",
    "            aligned_img_tensor_r = aligned_img_tensor.view(aligned_img_tensor.shape[0],aligned_img_tensor.shape[2]**2)\n",
    "\n",
    "            loss_orig = mmd_loss(originals_r.float(),outputs_orig_r.float()) * theta1 #torch.mean((torch.abs(outputs_orig - originals)))  \n",
    "\n",
    "            loss_align_r = mmd_loss(output_aligned_r.float(),aligned_img_tensor_r.float()) * theta1  #torch.mean((torch.abs(outputs_align - aligned_img_tensor)))\n",
    "\n",
    "            # loss\n",
    "            loss_diff = loss_orig - loss_align_r\n",
    "            tr_discriminator_losses.append(loss_diff)\n",
    "            loss_diff.backward(retain_graph=True)\n",
    "            discriminator_optimiser.step()\n",
    "\n",
    "        # output metrics...\n",
    "        accuracy = float(correct/ size)\n",
    "        print(accuracy)\n",
    "        avg_aligner_loss = float(sum(tr_aligner_losses) / len(tr_aligner_losses))\n",
    "        avg_disc_loss = float(sum(tr_discriminator_losses) / len(tr_discriminator_losses))\n",
    "        avg_cl_loss = float(sum(tr_classifier_losses) / len(tr_classifier_losses))\n",
    "\n",
    "        aligner_train_losses.append(avg_aligner_loss)\n",
    "        aligner_train_accuracy.append(accuracy * 100)\n",
    "        #print(f'Train Epoch: [{epoch}] Align-loss: {loss_align} Accuracy: {accuracy} ({correct}/{size})')\n",
    "\n",
    "\n",
    "    ####################\n",
    "    target_loader_v = torch.utils.data.DataLoader(target_test_dataset, num_workers=2, batch_size=len(target_test_dataset))\n",
    "\n",
    "    data_target = iter(target_loader_v).next()\n",
    "    inputs_t, labels_t = data_target\n",
    "\n",
    "    inputs = inputs_t\n",
    "\n",
    "    AE_model.eval()\n",
    "    classifier.eval()\n",
    "    aligner.eval()\n",
    "    discriminator.eval()\n",
    "\n",
    "    # classified aligned images\n",
    "    aligned = aligner(inputs)\n",
    "    outputs = AE_model(aligned)\n",
    "    cl_outputs = classifier(AE_model.latent.clone())\n",
    "    _, predicted = cl_outputs.max(1)\n",
    "    correct = (predicted == labels_t).sum().item()\n",
    "    size = labels_t.size(0)\n",
    "    accuracy = float(correct / size)\n",
    "    print(accuracy)\n",
    "\n",
    "     # classified raw(non-aligned) images\n",
    "    raw_outputs = AE_model(inputs)\n",
    "    raw_cl_outputs = classifier(AE_model.latent.clone())\n",
    "    _, raw_predicted = raw_cl_outputs.max(1)\n",
    "    raw_correct = (raw_predicted == labels_t).sum().item()\n",
    "    raw_size = labels_t.size(0)\n",
    "    raw_accuracy = float(raw_correct / raw_size)\n",
    "\n",
    "    print(raw_accuracy)\n",
    "\n",
    "    overall_accuracy.append(('Aligned Images', accuracy, 'Non-aligned images', raw_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Aligned Images',\n",
       "  0.5624336752741422,\n",
       "  'Non-aligned images',\n",
       "  0.5903784931022286),\n",
       " ('Aligned Images',\n",
       "  0.5571276972055182,\n",
       "  'Non-aligned images',\n",
       "  0.5981605942695437),\n",
       " ('Aligned Images',\n",
       "  0.5758754863813229,\n",
       "  'Non-aligned images',\n",
       "  0.6229218252564556),\n",
       " ('Aligned Images',\n",
       "  0.5175097276264592,\n",
       "  'Non-aligned images',\n",
       "  0.6027591085956845),\n",
       " ('Aligned Images',\n",
       "  0.5737530951538734,\n",
       "  'Non-aligned images',\n",
       "  0.6108949416342413)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(overall_accuracy).to_csv(\"ADAN_MMD_3days_1days.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
